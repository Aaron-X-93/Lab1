---
title: "Final_Project"
author: "Aaron"
date: "5/8/2020"
output:
  pdf_document: default
---

```{r,results='hide', warning = F}
library(fpc)           # for clustering and cluster validation.
library(data.table)    # for data processing
library(ggplot2)       # for data visulaization
library(randomForest)  # random forest modeling
#library(rgl)           # for plot3d
#library(FactoMineR)    # PCA with mixture data
library(factoextra)    # clustering visualization
library(caret)         # createDataPartition
library(pROC)          # AUC
#library(car)           # QQ plot
```

### Input data
```{r}
fulldata=read.csv(file.choose(), header = TRUE, stringsAsFactors = T, na.strings = c("NA","","#NA"))
```

#### Select final columns
```{r}
healthdata<-subset(fulldata, select=-c(tulineno,eeincome1,erhhch,erincome,erspemch,ethgt,etwgt,eudrink,eueat,euexercise,
          eufastfd,eufdsit,eufinlwgt,eusnap,eugenhth,euhgt,euinclvl,euincome2,eustores,eustreason,
          eutherm,euwgt,euwic,exincome1,eumilk,euffyday))
data1<-subset(healthdata,erbmi!=-1)
```

## Data preparation

### prepare to merge dietsoda data and soda data
```{r}
table(data1$eudietsoda,data1$eusoda,dnn = c('dietsoda','soda'))
```

```{r}
# set 0 under eudietsoda for not drink any
data1$eudietsoda[data1$eusoda==2] <- "0"
# there were -2 and -3 in dietsoda, coding those to -1 as missing group
data1$eudietsoda[data1$eudietsoda==-2|data1$eudietsoda==-3] <- "-1"
# coding those who drink both to diet, since we just want to know if they drink diet or not
# & sample size is very small
data1$eudietsoda[data1$eudietsoda==3] <- "2"
table(data1$eudietsoda,data1$eusoda,dnn = c('dietsoda','soda'))
```

```{r}
# recoding remaining with -1 to 0 for no
data1$euexfreq[data1$euexfreq==-1]<-0
data1$eufastfdfrq[data1$eufastfdfrq==-1]<-0
data1$eugroshp[data1$eugroshp==-1]<-0
data1$eumeat[data1$eumeat==-1]<-0
data1$euprpmel[data1$euprpmel==-1]<-0

# all -2 or -3 are actually missing so code to missing
data1$ertpreat[data1$ertpreat<0]<-NA
data1$ertseat[data1$ertseat<0]<-NA
data1$euexfreq[data1$euexfreq==-2|data1$euexfreq==-3]<-NA
data1$eufastfdfrq[data1$eufastfdfrq==-2|data1$eufastfdfrq==-3]<-NA
data1$eugroshp[data1$eugroshp==-2|data1$eugroshp==-3]<-NA
data1$eumeat[data1$eumeat==-2|data1$eumeat==-3]<-NA
data1$euprpmel[data1$euprpmel==-2|data1$euprpmel==-3]<-NA
```

```{r}
# For continuous variables, replacing missing with mean
data1$ertpreat[is.na(data1$ertpreat)]<- mean(data1$ertpreat, na.rm = TRUE)
data1$ertseat[is.na(data1$ertseat)] <- mean(data1$ertseat, na.rm = TRUE)
data1$euexfreq[is.na(data1$euexfreq)] <- mean(data1$euexfreq, na.rm = TRUE)
data1$eufastfdfrq[is.na(data1$eufastfdfrq)] <- mean(data1$eufastfdfrq, na.rm = TRUE)

# For categorical variables, replacing with mode
data1$eugroshp[is.na(data1$eugroshp)] <- 1
data1$eumeat[is.na(data1$eumeat)] <- 1
data1$euprpmel[is.na(data1$euprpmel)] <- 1
```

```{r}
colSums(is.na(data1))
```

```{r}
str(data1)
```

### convert some of the attitudes to factor and/or chr type
```{r}
data1$tucaseid = as.character(data1$tucaseid)
for (i in c(5,8,9,10,11)){
  data1[,i] <- as.factor(data1[,i])
}
``` 

```{r}
summary(data1)
str(data1)
```

### scaling the numerical variables
```{r}
library(scales) # Scaling the data
data_cluster <- subset(data1, select = -c(eusoda))
data_cluster[,3] <- rescale(data_cluster[,3])
data_cluster[,4] <- rescale(data_cluster[,4])
data_cluster[,6] <- rescale(data_cluster[,6])
data_cluster[,7] <- rescale(data_cluster[,7])
``` 

```{r}
summary(data_cluster)
``` 

## Clustering
```{r}
library(cluster)  # calculate gower distance
```
The data we prepared for clustering is a mixture of numerical and categorical variables. In order to calculate a dissimilarity matrix in this case, we go for something called Gower distance.
```{r}
gower.dist <- daisy(data_cluster[,3:10], metric = c("gower"))
```

### using gower distance to do the hierarchical clustering
```{r}
h_clust <- hclust(gower.dist, method = "complete") # clustering
#plot(h_clust, labels = data_cluster$tucaseid)      # dendrogram
#rect.hclust(h_clust,k=4) # set number of groups
```

```{r}
library(dendextend) # colorful h cluster picture
avg_dend_obj <- as.dendrogram(h_clust)
avg_col_dend <- color_branches(avg_dend_obj, k = 2)
plot(avg_col_dend)
```

### after hierarchical clustering, we are able to choose the number of groups we would like to have
```{r}
# check elbow in Agglomerative clustering
fviz_nbclust(data_cluster[,3:10], FUN = hcut, method = "wss", diss = gower.dist)
```
We see 2 or 3 seems like a better choice.

```{r}
# Check best k using average Silhouette Method
fviz_nbclust(data_cluster[3:10], FUN = hcut, method = "silhouette", diss = gower.dist)
```
We see 2 maybe a good choice.

```{r}
group_hc <- cutree(h_clust, k=2)  # set group number K
```


## using k-prototype to do the clustering and compare with hierarchical clustering result
K-Prototype is a simple combination of K-Means and K-Modes in clustering mixed attributes.

```{r}
library(clustMixType)  # fork-prototype
```

```{r}
# lets cluster the data to 2 groups, and let the algrithem select the lambda automatically
kpres <- kproto(data_cluster[,3:10], 2, iter.max=1000, nstart=10, 
                na.rm=TRUE, keep.data=TRUE, verbose=TRUE)
```

### check the correlation between the two clustering methods' results
```{r}
cor(group_hc, kpres$cluster)
```
As we can see that the 2 clustering results are highly related.

### visually check the distribution of the variables with the clustering result
```{r}
clprofiles(kpres, data_cluster[,2:10])
```

```{r}
comb1 <- cbind(data_cluster, cluster = as.factor(group_hc))
```

```{r}
# Visulize the erbmi distribution with cluster results
ggplot(data = comb1, mapping = aes(erbmi, fill=factor(cluster)))+
  geom_histogram(position = 'dodge', binwidth = 1,  alpha=I(0.5))
ggplot(comb1, aes(x = erbmi, colour = factor(cluster))) + geom_density()
```


## Training and testing data preparation for randomforest and logistic regression model
```{r}
# fix random seed
set.seed(28)

data_model = data_cluster
data_model$erbmi[data_model$erbmi<25] <- 0
data_model$erbmi[data_model$erbmi>=25] <- 1
data_model = subset(data_model, select = -c(tucaseid))
str(data_model)
```

### split to train and test data
```{r}
train_id <- createDataPartition(y= data_model$erbmi, p=0.8, list = FALSE)
training1 <- data_model[train_id,]
testing1 <- data_model[-train_id,]
prop.table((table(training1$erbmi)))
prop.table((table(testing1$erbmi)))
```

```{r}
comb_chi <- cbind(data_model, cluster = as.factor(group_hc))
```

```{r}
chisq <- chisq.test(table(comb_chi$cluster, comb_chi$erbmi))
table(comb_chi$cluster, comb_chi$erbmi)
chisq$observed
round(chisq$expected,2)
library(corrplot)
corrplot(chisq$residuals, is.cor = FALSE)
chisq
```


## Logistic regression model

### set factor variables to dummy variables
```{r}
dmy_soda <- model.matrix(~eudietsoda, training1)
dmy_groshp <- model.matrix(~eugroshp, training1)
dmy_meat <- model.matrix(~eumeat, training1)
dmy_prpmel <- model.matrix(~euprpmel, training1)
```

```{r}
model_lr1 <- glm(erbmi ~., data = training1, family = "binomial")
model_lr2 <- glm(erbmi ~ ertpreat+eudietsoda+euexfreq+eufastfdfrq, data = training1, family = "binomial")
```

```{r}
summary(model_lr1)
summary(model_lr2)
```

### set factor variables to dummy variables
```{r}
dmy_soda <- model.matrix(~eudietsoda, testing1)
dmy_groshp <- model.matrix(~eugroshp, testing1)
dmy_meat <- model.matrix(~eumeat, testing1)
dmy_prpmel <- model.matrix(~euprpmel, testing1)
```

```{r}
Prob_lm <- predict(object = model_lr2, newdata = testing1, type = "response")
Pred_lm <- ifelse(Prob_lm >= 0.66, "yes", "no")
```

```{r}
Pred_lm <- factor(Pred_lm, levels = c("no","yes"), order=TRUE)
f <- table(testing1$erbmi, Pred_lm)
f
```

```{r}
(503+621)/length(testing1$erbmi)
```

### Accracy and ROC
```{r}
library(ROCR) # for prediction()
auc(testing1$erbmi, Pred_lm)
pr_lr <- prediction(as.numeric(Pred_lm), as.numeric(testing1$erbmi))
perf_lr <- performance(pr_lr, measure = "tpr", x.measure = "fpr")
plot(perf_lr)
```


## RF


```{r}
str(training1)
```

### convert variables to factor type
```{r}
training = training1
testing = testing1
training[,1] <- as.factor(training[,1])
testing[,1] <- as.factor(testing[,1])
```

```{r}
#library(randomForest)
mod_rf <- randomForest(erbmi ~ ., method = "class", data = training, 
                       na.action=na.fail, nodesize=1, importance=TRUE, ntree=200, mtry=2)
```

### try to decide ntree number
```{r}
plot(mod_rf)  # in this case, it is 200
```

### try to decide mtry number
```{r}
n <- ncol(training1) -1
errRate <- c(1)
for (i in 1:n){
  m <- randomForest(erbmi ~ ., method = "class", data = training, mtry=i, ntree = 200)  
  err<-mean(m$err.rate)  
  print(err)  
  }
```
in this case, mtry = 2 is the best.

### check the importance of the dataset attributes for the prediction
```{r}
varImpPlot(mod_rf, main="")
```

### predict on the testing data
```{r}
Pred_rf <- predict(mod_rf, testing)
table(testing$erbmi, Pred_rf, dnn = c('Actual','Predicted'))
cor(as.numeric(Pred_rf),as.numeric(testing$erbmi))
```

### AUC and Accuracy
```{r}
(86+1253)/length(testing$erbmi)
auc(testing$erbmi, as.numeric(Pred_rf))
```
